{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UAWAh3vsVMKQ",
        "ZjKMbBeY7x_Y",
        "dM0rPbmL6pIZ",
        "5JEeOwtA6s9J",
        "s0BG2wu87Dpz",
        "jsF_iU7r7YTN",
        "Qewgtd2a786x",
        "ikMiY9dzjOAr",
        "JoCEMTRc8J3f",
        "68nLL_iR8TGl",
        "giHun4fm8uRb",
        "B0kPrEEX9IBB",
        "W2rP9pP29pR_",
        "iyKDkngR-IHM",
        "mNyXqFNiOcAN",
        "gRFN0wwmWfHQ",
        "ojGr_KgcWpp1",
        "BWzrT7c8zVxi",
        "eH0ydBl5X_d4",
        "SNOpt2-V5foO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Updating libraries to avoid compatibility issues\n",
        "Always run at the start of runtime"
      ],
      "metadata": {
        "id": "UAWAh3vsVMKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.8.0"
      ],
      "metadata": {
        "id": "Kud5qs0IoQws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx==2.8.8"
      ],
      "metadata": {
        "id": "T4sVTGEWgsK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "ZjKMbBeY7x_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up"
      ],
      "metadata": {
        "id": "dM0rPbmL6pIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgjlAwWa6dU_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TOKEN'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UtbfVisWflXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/\"\n",
        "error_log_path = \"/content/\""
      ],
      "metadata": {
        "id": "-M7lolr___Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests \n",
        "import pandas as pd \n",
        "import time"
      ],
      "metadata": {
        "id": "sMtFcHBN620F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auth"
      ],
      "metadata": {
        "id": "5JEeOwtA6s9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_headers(bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    return headers\n",
        "headers = create_headers(os.environ['TOKEN'])"
      ],
      "metadata": {
        "id": "IEMVLDwd69YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Query"
      ],
      "metadata": {
        "id": "s0BG2wu87Dpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_url(query, start_time, end_time, max_results, expansions, tweet_fields, user_fields, place_fields, endpoint):\n",
        "    \n",
        "    search_url = endpoint #Change to the endpoint you want to collect data from\n",
        "\n",
        "    #change params based on the endpoint you are using\n",
        "    #also can request different fields, e.g ids of users ... \n",
        "    query_params = {'query': query,\n",
        "                    'end_time': end_time,\n",
        "                    'start_time': start_time,\n",
        "                    'max_results': max_results,\n",
        "                    'expansions': expansions,\n",
        "                    'tweet.fields': tweet_fields,\n",
        "                    'user.fields': user_fields,\n",
        "                    'place.fields': place_fields}\n",
        "\n",
        "    return (search_url, query_params)"
      ],
      "metadata": {
        "id": "3RcIdxOz7AdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connect_to_endpoint(url, headers, params, next_token = None):\n",
        "    #only change the default value of next_token if it is a real value returned in the response\n",
        "    if next_token is not None and next_token != '':\n",
        "      params['next_token'] = next_token\n",
        "    #create a \"GET\" request to the specified url, add headers and parameters\n",
        "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
        "    if response.status_code != 200:\n",
        "        #if something goes wrong, we need to know\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    #otherwise, we want the payload of our response, which contains our tweet(s)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "Gzuw8G6r7JJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(query, start_time, end_time, max_results, expansions, tweet_fields, user_fields, place_fields, endpoint, next_token=\"\"):\n",
        "  \n",
        "  results = []\n",
        "\n",
        "\n",
        "  while next_token is not None:\n",
        "    try:    \n",
        "      url = create_url(query, start_time, end_time, max_results, expansions, tweet_fields, user_fields, place_fields, endpoint)\n",
        "      json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
        "      #if we have results, they will be in the field 'data' of our response\n",
        "      if \"data\" in json_response:\n",
        "        results.extend(json_response[\"data\"])\n",
        "        print(str(len(json_response[\"data\"])) + \" Tweets downloaded in this batch.\")\n",
        "      #the next_token is added to the field 'meta' of our response\n",
        "      if \"meta\" in json_response:\n",
        "        if \"next_token\" in json_response[\"meta\"].keys():\n",
        "          next_token = json_response[\"meta\"][\"next_token\"]          \n",
        "        else:\n",
        "          next_token = None\n",
        "      else:\n",
        "        next_token = None\n",
        "\n",
        "      \n",
        "      #to control the rate limit we need to slow down our download\n",
        "      time.sleep(3)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Error occured\", e)\n",
        "      print(\"Next token value\", next_token)\n",
        "      error_log = {\"Error\":e, \"Next token\":next_token, \"Day\":start_time, \n",
        "                   \"Downloaded\":len(results)}\n",
        "      pd.DataFrame.from_dict(error_log, orient=\"index\").to_csv(error_log_path+query+\"_\"+start_time+\"_\"+next_token+\".csv\")\n",
        "      return(results, next_token)\n",
        "\n",
        "  print(\"Done\")\n",
        "  \n",
        "  return (results, next_token)"
      ],
      "metadata": {
        "id": "VgO0R1Ld7LAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download And Save"
      ],
      "metadata": {
        "id": "jsF_iU7r7YTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = \"2022-11-25T13:00:00.000Z\"\n",
        "end_time = \"2022-11-25T13:00:20.000Z\"\n",
        "query_text = \"(#democrat OR #democrats OR #republican OR #republicans) -is:reply -is:retweet\"\n",
        "endpoint = \"https://api.twitter.com/2/tweets/search/all/\"\n",
        "path = \"/content/\"\n",
        "max_results = 500\n",
        "no_days = 15"
      ],
      "metadata": {
        "id": "ZkoJQYy67ONQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = get_data(query_text, start_time = start_time, end_time = end_time, \n",
        "          max_results=max_results, expansions='author_id,in_reply_to_user_id,geo.place_id', \n",
        "          tweet_fields='id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
        "          user_fields='id,name,username,created_at,description,public_metrics,verified',\n",
        "          place_fields='full_name,id,country,country_code,geo,name,place_type',\n",
        "          endpoint=endpoint)[0]          \n",
        "tweets_df = pd.DataFrame(tweets)\n",
        "tweets_df.to_pickle(path+\"_tweets.pkl\")"
      ],
      "metadata": {
        "id": "yLUAu_T37eaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Data"
      ],
      "metadata": {
        "id": "Qewgtd2a786x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload"
      ],
      "metadata": {
        "id": "ikMiY9dzjOAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path = \"/content/\"\n",
        "error_log_path = \"/content/\""
      ],
      "metadata": {
        "id": "_4HM4mBFjFd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.read_pickle(path+\"tweets.pkl\")"
      ],
      "metadata": {
        "id": "tn87wHd07gQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df"
      ],
      "metadata": {
        "id": "CC9qWOM4y5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "JoCEMTRc8J3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered = tweets_df.copy() #it's a good idea to work on the copy of original dataframe, so we can always go back to it if we mess something up\n",
        "column_list = [\"id\",\"author_id\",\"created_at\", \"text\",\"entities\",\"public_metrics\", \"lang\"]\n",
        "tweets_filtered = tweets_filtered[column_list]"
      ],
      "metadata": {
        "id": "Pozo_sYV8GOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered"
      ],
      "metadata": {
        "id": "wF2YAF918NXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis Addition"
      ],
      "metadata": {
        "id": "68nLL_iR8TGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "BfuuVXqY8O5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner_sentiment(tweet):\n",
        "    tweet = re.sub(\"@\\w+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#\\w+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "DaQpjJ948qpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered['sentiment'] = tweets_filtered['text'].apply(lambda testo: sid.polarity_scores(cleaner_sentiment(testo))['compound'])"
      ],
      "metadata": {
        "id": "KXe3S87c8sNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Words/Hashtags"
      ],
      "metadata": {
        "id": "giHun4fm8uRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK tools\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from collections import defaultdict\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "tokenizer = nltk.RegexpTokenizer(r'\\w+')"
      ],
      "metadata": {
        "id": "XFIsa5T18zv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.append('amp')"
      ],
      "metadata": {
        "id": "1L3F1y6Xmmj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(tweet):\n",
        "    tweet = re.sub(\"@\\w+\",\"\",tweet) # remove mentions\n",
        "    tweet = re.sub(\"#\\w+\", \"\",tweet) # remove hashtags\n",
        "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) # remove http links\n",
        "    tweet = \" \".join(tweet.split())\n",
        "    tweet = \" \".join(w for w in tokenizer.tokenize(tweet) if ((not w.lower() in stop_words) and len(w)>1 ))\n",
        "    #remove stop words\n",
        "    lemma_function = WordNetLemmatizer()\n",
        "    tweet = \" \".join(lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(tweet))) #lemmatize\n",
        "    tweet = str.lower(tweet) #to lowercase\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "F24M-ZKq84Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered[\"clean_text\"] = tweets_filtered[\"text\"].map(cleaner)"
      ],
      "metadata": {
        "id": "LbxMCvYX858b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered"
      ],
      "metadata": {
        "id": "WraNSwda88Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered.loc[tweets_filtered[\"clean_text\"].isnull(),\"clean_text\"] = \"\""
      ],
      "metadata": {
        "id": "q5sXve638_Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize an empty dict\n",
        "unique_words = {}\n",
        "unique_weights = {}\n",
        "\n",
        "for idx, row in tweets_filtered.iterrows():\n",
        "  if row[\"clean_text\"] != \"\":\n",
        "    for word in tokenizer.tokenize(row[\"clean_text\"]):\n",
        "      unique_words.setdefault(word,0)\n",
        "      unique_words[word] += 1\n",
        "      unique_weights.setdefault(word,float(0))\n",
        "      unique_weights[word] += float(row[\"sentiment\"])"
      ],
      "metadata": {
        "id": "4TRz5gB49AkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uw_df = pd.DataFrame.from_dict(unique_words, orient='index').reset_index()\n",
        "uw_df.rename(columns = {'index':'Word', 0:'Count'}, inplace=True)\n",
        "uw_df['Sentiment'] = uw_df['Word'].apply(lambda word: unique_weights[word])/uw_df['Count']\n",
        "uw_df.sort_values(by=['Count'], ascending=False, inplace=True)\n",
        "uw_df = uw_df.reset_index().drop(columns=[\"index\"])\n",
        "uw_df['Sentiment'] = ((uw_df['Sentiment'])**(1/2)).fillna(0)-((-uw_df['Sentiment'])**(1/2)).fillna(0)"
      ],
      "metadata": {
        "id": "O2_b8faw9Cq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uw_df"
      ],
      "metadata": {
        "id": "HR-55j-A9EQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Hashtags"
      ],
      "metadata": {
        "id": "B0kPrEEX9IBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered.loc[tweets_df[\"entities\"].isnull(), \"entities\"] = None\n",
        "tweets_filtered[\"hashtags\"] = \"\""
      ],
      "metadata": {
        "id": "35HFWjjN9MKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_hashtags = {}\n",
        "unique_hweights = {}\n",
        "index = 0\n",
        "\n",
        "for idx, row in tweets_filtered.iterrows():\n",
        "  if row[\"entities\"] is not None and \"hashtags\" in row[\"entities\"]:\n",
        "    hl = []\n",
        "    for hashtag in row[\"entities\"][\"hashtags\"]:\n",
        "      tag = '#' + hashtag[\"tag\"].lower()\n",
        "      unique_hashtags.setdefault(tag, 0)\n",
        "      unique_hashtags[tag] += 1\n",
        "      hl.append(tag)\n",
        "      unique_hweights.setdefault(tag,float(0))\n",
        "      unique_hweights[tag] += float(row[\"sentiment\"])\n",
        " \n",
        "    tweets_filtered.at[idx,\"hashtags\"] = hl"
      ],
      "metadata": {
        "id": "uHFI5Oyw9ai8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_hashtags = dict(sorted(unique_hashtags.items(), key=lambda item: item[1], reverse=True))"
      ],
      "metadata": {
        "id": "nKas9RM59gOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uh_df = pd.DataFrame.from_dict(unique_hashtags, orient='index').reset_index()\n",
        "uh_df.rename(columns = {'index':'Hashtag', 0:'Count'}, inplace=True)\n",
        "uh_df['Sentiment'] = uh_df['Hashtag'].apply(lambda tag: unique_hweights[tag])/uh_df['Count']\n",
        "uh_df['Sentiment'] = ((uh_df['Sentiment'])**(1/2)).fillna(0)-((-uh_df['Sentiment'])**(1/2)).fillna(0)"
      ],
      "metadata": {
        "id": "v792_ANT9ht4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uh_df"
      ],
      "metadata": {
        "id": "gqrTcWrn9jFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Network"
      ],
      "metadata": {
        "id": "W2rP9pP29pR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "7fjOrIqV9r8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uh = unique_hashtags.keys()\n",
        "uw = unique_words.keys()"
      ],
      "metadata": {
        "id": "zTgsrU1w9tw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = {}\n",
        "hnetwork = {}\n",
        "wnetwork = {}\n",
        "network_key = 0\n",
        "for index, row in tweets_filtered.iterrows():\n",
        "    combined_list = [hashtag for hashtag in row[\"hashtags\"]] + [word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
        "    #itertool product creates Cartesian product of each element in the combined list\n",
        "    for pair in itertools.product(combined_list, combined_list):\n",
        "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
        "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
        "            network.setdefault(pair,0)\n",
        "            network[pair] += 1\n",
        "    hashtag_list = [hashtag for hashtag in row[\"hashtags\"]]\n",
        "    for pair in itertools.product(hashtag_list, hashtag_list):\n",
        "        if pair[0]!=pair[1] and not(pair[::-1] in hnetwork):\n",
        "            hnetwork.setdefault(pair,0)\n",
        "            hnetwork[pair] += 1\n",
        "    word_list = [word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
        "    for pair in itertools.product(word_list, word_list):\n",
        "        if pair[0]!=pair[1] and not(pair[::-1] in wnetwork):\n",
        "            wnetwork.setdefault(pair,0)\n",
        "            wnetwork[pair] += 1\n",
        "\n",
        "network_df = pd.DataFrame.from_dict(network, orient=\"index\")\n",
        "hnetwork_df = pd.DataFrame.from_dict(hnetwork, orient=\"index\")\n",
        "wnetwork_df = pd.DataFrame.from_dict(wnetwork, orient=\"index\")"
      ],
      "metadata": {
        "id": "Vw9soV8K9wkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_df.reset_index(inplace=True)\n",
        "network_df.columns = [\"pair\",\"weight\"]\n",
        "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "network_df"
      ],
      "metadata": {
        "id": "Rqoiklji9xDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hnetwork_df.reset_index(inplace=True)\n",
        "hnetwork_df.columns = [\"pair\",\"weight\"]\n",
        "hnetwork_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "hnetwork_df"
      ],
      "metadata": {
        "id": "y5pmevla92ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnetwork_df.reset_index(inplace=True)\n",
        "wnetwork_df.columns = [\"pair\",\"weight\"]\n",
        "wnetwork_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
        "wnetwork_df"
      ],
      "metadata": {
        "id": "93GcclxU93jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
        "up_weighted = []\n",
        "for edge in network:\n",
        "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "    #if(network[edge])>1:\n",
        "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
        "\n",
        "hup_weighted = []\n",
        "for edge in hnetwork:\n",
        "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "    #if(network[edge])>1:\n",
        "    hup_weighted.append((edge[0],edge[1],hnetwork[edge]))\n",
        "\n",
        "wup_weighted = []\n",
        "for edge in wnetwork:\n",
        "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
        "    #if(network[edge])>1:\n",
        "    wup_weighted.append((edge[0],edge[1],wnetwork[edge]))\n",
        "\n",
        "\n",
        "G = nx.Graph()\n",
        "hG = nx.Graph()\n",
        "wG = nx.Graph()\n",
        "G.add_weighted_edges_from(up_weighted)\n",
        "hG.add_weighted_edges_from(hup_weighted)\n",
        "wG.add_weighted_edges_from(wup_weighted)"
      ],
      "metadata": {
        "id": "_KWf07AH96aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(G.nodes()))\n",
        "print(len(G.edges()))\n",
        "print(len(hG.nodes()))\n",
        "print(len(hG.edges()))\n",
        "print(len(wG.nodes()))\n",
        "print(len(wG.edges()))"
      ],
      "metadata": {
        "id": "lgctevxk98Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nx.write_gpickle(G,path+\"network.pkl\")\n",
        "nx.write_gpickle(hG,path+\"hnetwork.pkl\")\n",
        "nx.write_gpickle(wG,path+\"wnetwork.pkl\")"
      ],
      "metadata": {
        "id": "yAq6EODRUykj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = path+\"/edgelist.csv\"\n",
        "nx.write_weighted_edgelist(G, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on collab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./edgelist.csv"
      ],
      "metadata": {
        "id": "ZwD9zCwCU4YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = path+\"/hedgelist.csv\"\n",
        "nx.write_weighted_edgelist(hG, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on collab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./hedgelist.csv"
      ],
      "metadata": {
        "id": "23y4yyONU51z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = path+\"/wedgelist.csv\"\n",
        "nx.write_weighted_edgelist(wG, filename, delimiter=\",\")\n",
        "#add header with appropriate column names (works on collab and Linux/Mac(?))\n",
        "!sed -i.bak 1i\"Source,Target,Weight\" ./wedgelist.csv"
      ],
      "metadata": {
        "id": "iXnxCaqUU7Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Political Affiliation Addition"
      ],
      "metadata": {
        "id": "iyKDkngR-IHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uw_df['Political'] = float(0)\n",
        "uh_df['Political'] = float(0)\n",
        "uh_df['Political'][uh_df['Hashtag']=='#democrats'] = -1\n",
        "uh_df['Political'][uh_df['Hashtag']=='#democrat'] = -1\n",
        "uh_df['Political'][uh_df['Hashtag']=='#republicans'] = 1\n",
        "uh_df['Political'][uh_df['Hashtag']=='#republican'] = 1"
      ],
      "metadata": {
        "id": "ZTv4pQle-L6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_nodes = uw_df.copy()\n",
        "word_nodes[\"Label\"] = word_nodes[\"Word\"]\n",
        "word_nodes.rename(columns={\"Word\":\"Id\"},inplace=True)\n",
        "word_nodes = word_nodes[['Id','Label','Count','Sentiment','Political']]\n",
        "\n",
        "word_nodes"
      ],
      "metadata": {
        "id": "Lk1r2edzLAkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_nodes = uh_df.copy()\n",
        "hashtag_nodes[\"Label\"] = hashtag_nodes[\"Hashtag\"]\n",
        "hashtag_nodes.rename(columns={\"Hashtag\":\"Id\"},inplace=True)\n",
        "hashtag_nodes = hashtag_nodes[['Id','Label','Count','Sentiment','Political']]\n",
        "\n",
        "hashtag_nodes"
      ],
      "metadata": {
        "id": "k2y2JOlELCKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindex_order = []\n",
        "for node in list(hG.nodes()):\n",
        "  hindex_order.append(hashtag_nodes.index[hashtag_nodes['Id'] == node].tolist()[0])"
      ],
      "metadata": {
        "id": "9hkrXTuQWEzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_nodes = hashtag_nodes.reindex(hindex_order)"
      ],
      "metadata": {
        "id": "etQfGOXWWRxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_nodes"
      ],
      "metadata": {
        "id": "aiJ_USP_Xtd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PLMP"
      ],
      "metadata": {
        "id": "mNyXqFNiOcAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from networkx.linalg.graphmatrix import adjacency_matrix\n",
        "hM = adjacency_matrix(hG)"
      ],
      "metadata": {
        "id": "Gt9WxzlI-ZRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "rnhM = normalize(hM, axis=1, norm='l1').todense()"
      ],
      "metadata": {
        "id": "giKu66xXFSzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodelist.to_csv(\"nodelist.csv\",index=False)"
      ],
      "metadata": {
        "id": "8jhkoT1-TKgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hrnM.toarray()"
      ],
      "metadata": {
        "id": "6cKoJo5wUbNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.savetxt(\"adj_matrix.csv\", rnM.toarray(), delimiter=\",\")"
      ],
      "metadata": {
        "id": "i60WhWnfTZdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hcopy = np.transpose(np.matrix(hashtag_nodes.copy()['Political']))\n",
        "i = 0\n",
        "diff = 1\n",
        "while not(i>1000 or diff<(10**-6)):\n",
        "  temphcopy = 0.80*np.dot(rnhM,hcopy)+0.2*np.transpose(np.matrix(hashtag_nodes['Political']))\n",
        "  diff = np.nansum(np.abs(temphcopy-hcopy), dtype=np.float64)\n",
        "  hcopy = temphcopy\n",
        "  i+=1\n",
        "hcopy"
      ],
      "metadata": {
        "id": "oWybSD3GOfVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = hashtag_nodes.copy()"
      ],
      "metadata": {
        "id": "_sfARRl3bNJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'] = np.transpose(hcopy).tolist()[0]"
      ],
      "metadata": {
        "id": "jIkqKUvXbR1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'].max()"
      ],
      "metadata": {
        "id": "28F5Jqn-c3kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'].mean()"
      ],
      "metadata": {
        "id": "cOSU12jqqXq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.sort_values(by=\"Political\",inplace=True, ascending=False)"
      ],
      "metadata": {
        "id": "MDf9tbszcAmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wm = (a['Political']*a['Count']).sum()/(a['Count'].sum())"
      ],
      "metadata": {
        "id": "aaaPXcdd0D-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wm"
      ],
      "metadata": {
        "id": "TVdggJtg0WJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'] = a['Political'].apply(lambda row: ((row)>=0)*((row)/(hcopy.max()))+((row)<0)*((row)/(-hcopy.min())))"
      ],
      "metadata": {
        "id": "qbYXY9ybkWJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'].mean()"
      ],
      "metadata": {
        "id": "-E_yjUtHilgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'].min()"
      ],
      "metadata": {
        "id": "DqqPlvY1tcOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political']= ((a['Political'])**(1/2)).fillna(0)-((-a['Political'])**(1/2)).fillna(0)"
      ],
      "metadata": {
        "id": "YhSZ7ja-1jsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Alignment'] = a['Political']*a['Sentiment']"
      ],
      "metadata": {
        "id": "Vl1ARfRH0Gsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.sort_values(by=\"Count\",inplace=True, ascending=False)"
      ],
      "metadata": {
        "id": "y1k3ggkDTCkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_nodes = a"
      ],
      "metadata": {
        "id": "YFpsumrdTRAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(a['Political']*a['Count']).mean()/(a['Count'].mean())"
      ],
      "metadata": {
        "id": "hBQo44PXxrWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[0:50]"
      ],
      "metadata": {
        "id": "H4bj6wZGxjLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['Political'].plot(kind='hist')"
      ],
      "metadata": {
        "id": "cKG6LIfNw9EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expansion to tweets and words"
      ],
      "metadata": {
        "id": "gRFN0wwmWfHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered['political'] = float(0)\n",
        "htlist = a['Id'].tolist()\n",
        "for index, row in tweets_filtered.iterrows():\n",
        "  if row['hashtags']:\n",
        "    algn = float(0)\n",
        "    for ht in row['hashtags']:\n",
        "      if ht in htlist:\n",
        "        algn += float(a[a['Id']==ht]['Political'])\n",
        "    tweets_filtered.loc[index,'political'] = algn/len(row['hashtags'])"
      ],
      "metadata": {
        "id": "8bkBUZUB88jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_political = {}\n",
        "for idx, row in tweets_filtered.iterrows():\n",
        "  if row[\"clean_text\"] != \"\":\n",
        "    for word in tokenizer.tokenize(row[\"clean_text\"]):\n",
        "      unique_political.setdefault(word,float(0))\n",
        "      unique_political[word] += float(row[\"political\"])\n",
        "word_nodes['Political'] = word_nodes['Id'].apply(lambda word: unique_political[word])/word_nodes['Count']"
      ],
      "metadata": {
        "id": "giGcn2kNHd1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered['alignment'] = tweets_filtered['political']*tweets_filtered['sentiment']"
      ],
      "metadata": {
        "id": "MJrpFm8UUGlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_nodes['Alignment'] = word_nodes['Political']*word_nodes['Sentiment']"
      ],
      "metadata": {
        "id": "sZ27Y9-XUCBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nodelist output"
      ],
      "metadata": {
        "id": "ojGr_KgcWpp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist = hashtag_nodes.append(word_nodes, ignore_index=True)"
      ],
      "metadata": {
        "id": "g1ZNuwBUTtYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist.to_csv(\"nodelist.csv\",index=False)\n",
        "hashtag_nodes.to_csv(\"hashtaglist.csv\",index=False)\n",
        "word_nodes.to_csv(\"wordlist.csv\",index=False)"
      ],
      "metadata": {
        "id": "Kxjy0gDrTjIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodelist"
      ],
      "metadata": {
        "id": "MkrhMbS9Tub-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output tweets_filtered"
      ],
      "metadata": {
        "id": "BWzrT7c8zVxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_filtered.to_csv(path+\"tweets_filtered.csv\")"
      ],
      "metadata": {
        "id": "ACHMtxiMzUhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replies collection\n",
        "Uses functions from Data Collection and Working with Data"
      ],
      "metadata": {
        "id": "eH0ydBl5X_d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_ids = []\n",
        "for idx, row in tweets_filtered.iterrows():\n",
        "  if row['public_metrics']['reply_count']>5:\n",
        "      tweets_ids.append([row['id'],row['author_id']])"
      ],
      "metadata": {
        "id": "pdhv-FI1X-dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_replies(query, start_time, end_time, max_results, expansions, tweet_fields, user_fields, place_fields, endpoint, next_token=\"\"):\n",
        "  \n",
        "  results = []\n",
        "\n",
        "\n",
        "  while next_token is not None:\n",
        "    try:    \n",
        "      url = create_url(query, start_time, end_time, max_results, expansions, tweet_fields, user_fields, place_fields, endpoint)\n",
        "      json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
        "      #if we have results, they will be in the field 'data' of our response\n",
        "      if \"data\" in json_response:\n",
        "        results.extend(json_response[\"data\"])\n",
        "        print(str(len(json_response[\"data\"])) + \" Tweets downloaded in this batch.\")\n",
        "      #the next_token is added to the field 'meta' of our response\n",
        "      if \"meta\" in json_response:\n",
        "        if \"next_token\" in json_response[\"meta\"].keys():\n",
        "          next_token = json_response[\"meta\"][\"next_token\"]          \n",
        "        else:\n",
        "          next_token = None\n",
        "      else:\n",
        "        next_token = None\n",
        "\n",
        "      \n",
        "      #to control the rate limit we need to slow down our download\n",
        "      time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(\"Error occured\", e)\n",
        "      print(\"Next token value\", next_token)\n",
        "      error_log = {\"Error\":e, \"Next token\":next_token, \"Day\":start_time, \n",
        "                   \"Downloaded\":len(results)}\n",
        "      pd.DataFrame.from_dict(error_log, orient=\"index\").to_csv(error_log_path+query+\"_\"+start_time+\"_\"+next_token+\".csv\")\n",
        "      return(results, next_token)\n",
        "  \n",
        "  return (results, next_token)"
      ],
      "metadata": {
        "id": "0PfUp6pRpr_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = \"2022-02-01T13:00:00.000Z\"\n",
        "end_time = \"2023-01-11T13:00:20.000Z\"\n",
        "query_text = \"conversation_id:\"\n",
        "endpoint = \"https://api.twitter.com/2/tweets/search/all/\"\n",
        "path = \"/content/\"\n",
        "max_results = 100\n",
        "no_days = 15\n",
        "\n",
        "replies = pd.DataFrame(columns=tweets_df.columns)\n",
        "for tweet_id in tweets_ids:\n",
        "  query_text = \"in_reply_to_status_id:\"+str(tweet_id[0])+\" -from:\"+str(tweet_id[1])\n",
        "  replies_single = get_replies(query_text, start_time = start_time, end_time = end_time, \n",
        "          max_results=max_results, expansions='author_id,in_reply_to_user_id,geo.place_id', \n",
        "          tweet_fields='id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
        "          user_fields='id,name,username,created_at,description,public_metrics,verified',\n",
        "          place_fields='full_name,id,country,country_code,geo,name,place_type',\n",
        "          endpoint=endpoint)[0]          \n",
        "  replies_single_df = pd.DataFrame(replies_single)\n",
        "  replies = replies.append(replies_single_df, ignore_index=True)\n",
        "\n",
        "print(\"Done\")\n",
        "replies.to_pickle(path+\"replies.pkl\")"
      ],
      "metadata": {
        "id": "CLlO09B5QNbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies"
      ],
      "metadata": {
        "id": "Z8rpZO1Bn0i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Working with Replies Data"
      ],
      "metadata": {
        "id": "SNOpt2-V5foO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path = \"/content/\"\n",
        "error_log_path = \"/content/\""
      ],
      "metadata": {
        "id": "bRk600vVD_6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies = pd.read_pickle(path+\"replies.pkl\")"
      ],
      "metadata": {
        "id": "5rNn9yjkEJxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered = replies.copy() #it's a good idea to work on the copy of original dataframe, so we can always go back to it if we mess something up\n",
        "column_list = [\"id\",\"conversation_id\",\"author_id\",\"created_at\", \"text\",\"entities\",\"public_metrics\", \"lang\"]\n",
        "replies_filtered = replies_filtered[column_list]"
      ],
      "metadata": {
        "id": "jr0jyyJ33-Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered['sentiment'] = replies_filtered['text'].apply(lambda testo: sid.polarity_scores(cleaner_sentiment(testo))['compound'])"
      ],
      "metadata": {
        "id": "rP_gROnu4QUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered[\"clean_text\"] = replies_filtered[\"text\"].map(cleaner)\n",
        "replies_filtered.loc[replies_filtered[\"clean_text\"].isnull(),\"clean_text\"] = \"\""
      ],
      "metadata": {
        "id": "K5ZKQddn5F8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered.loc[replies[\"entities\"].isnull(), \"entities\"] = None\n",
        "replies_filtered[\"hashtags\"] = \"\""
      ],
      "metadata": {
        "id": "B__uNbTp7B_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_h = {}\n",
        "index = 0\n",
        "\n",
        "for idx, row in replies_filtered.iterrows():\n",
        "  if row[\"entities\"] is not None and \"hashtags\" in row[\"entities\"]:\n",
        "    hl = []\n",
        "    for hashtag in row[\"entities\"][\"hashtags\"]:\n",
        "      tag = '#' + hashtag[\"tag\"].lower()\n",
        "      unique_h.setdefault(tag, 0)\n",
        "      unique_h[tag] += 1\n",
        "      hl.append(tag)\n",
        " \n",
        "    replies_filtered.at[idx,\"hashtags\"] = hl"
      ],
      "metadata": {
        "id": "qDnc7koU8mHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered['political'] = float(0)\n",
        "for index, row in replies_filtered.iterrows():\n",
        "  check=False\n",
        "  algn = float(0)\n",
        "  cnt=0\n",
        "  if row['hashtags']:\n",
        "    for ht in row['hashtags']:\n",
        "      if ht in htlist:\n",
        "        cnt+=1\n",
        "        algn += float(a[a['Id']==ht]['Political'])\n",
        "  if (cnt==0 and row['clean_text'] != \"\"):\n",
        "    for word in str.split(row[\"clean_text\"], \" \"):\n",
        "      if word in list(word_nodes['Id']):\n",
        "        cnt+=1\n",
        "        algn += float(word_nodes[word_nodes['Id']==word]['Political'])\n",
        "  if cnt!= 0:\n",
        "    replies_filtered.loc[index,'political'] = algn/cnt"
      ],
      "metadata": {
        "id": "Wm0BEGr6zRLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered['alignment'] = replies_filtered['political']*replies_filtered['sentiment']"
      ],
      "metadata": {
        "id": "OgfXdMFvBg6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered"
      ],
      "metadata": {
        "id": "YrmPlXz8DgvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replies_filtered.to_csv(path+\"replies_filtered.csv\")"
      ],
      "metadata": {
        "id": "OTa_qA_9EAsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Average measurement for replies to tweets (neighbors)"
      ],
      "metadata": {
        "id": "oXjHxE5EL9h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_ids = []\n",
        "for idx, row in tweets_filtered.iterrows():\n",
        "  if row['public_metrics']['reply_count']>5:\n",
        "      tweets_ids.append([row['id'],row['author_id']])"
      ],
      "metadata": {
        "id": "keFpgBetFVnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neigh = pd.DataFrame(columns=['id','sentiment','political','alignment'])\n",
        "lista=[]\n",
        "for tweet_id in tweets_ids:\n",
        "  lista = list(replies_filtered[replies_filtered['conversation_id']==str(tweet_id[0])][['sentiment','political', 'alignment']].mean())\n",
        "  lista.append(tweet_id[0])\n",
        "  neigh = neigh.append(pd.DataFrame([lista], columns=[\"sentiment\",\"political\",\"alignment\",\"id\"]), ignore_index=True)"
      ],
      "metadata": {
        "id": "olR92SD_EV-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neigh"
      ],
      "metadata": {
        "id": "YazvcrfeH4hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neigh.to_csv(path+\"neigh.csv\")"
      ],
      "metadata": {
        "id": "kJQlfcNCMFZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
